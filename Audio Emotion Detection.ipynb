{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":234911,"sourceType":"datasetVersion","datasetId":99505},{"sourceId":1877714,"sourceType":"datasetVersion","datasetId":1118008},{"sourceId":53833937,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport librosa\nimport random\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\n# Dataset Path (Replace with actual path)\ndataset_folder = \"/kaggle/input/speech-emotion-recognition-en/Tess\"  # Folder containing subfolders of audio files\n\n# Function to load and extract features from audio\ndef extract_features(file_path, sr=22050, n_mfcc=40):\n    audio, _ = librosa.load(file_path, sr=sr)\n    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n    return mfcc.T  # Transpose to get (time_steps, features)\n\n# Prepare dataset\nfeatures, labels = [], []\nlabel_map = {}\nfor idx, emotion in enumerate(sorted(os.listdir(dataset_folder))):\n    emotion_folder = os.path.join(dataset_folder, emotion)\n    if os.path.isdir(emotion_folder):\n        label_map[idx] = emotion\n        for file in os.listdir(emotion_folder):\n            file_path = os.path.join(emotion_folder, file)\n            if file_path.endswith(\".wav\"):\n                mfcc = extract_features(file_path)\n                features.append(mfcc)\n                labels.append(idx)\n                ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:40:34.403967Z","iopub.execute_input":"2025-04-04T16:40:34.404182Z","iopub.status.idle":"2025-04-04T16:42:01.329961Z","shell.execute_reply.started":"2025-04-04T16:40:34.404161Z","shell.execute_reply":"2025-04-04T16:42:01.329012Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T16:44:08.578086Z","iopub.execute_input":"2025-04-04T16:44:08.578499Z","iopub.status.idle":"2025-04-04T16:44:08.583704Z","shell.execute_reply.started":"2025-04-04T16:44:08.578462Z","shell.execute_reply":"2025-04-04T16:44:08.582509Z"}},"outputs":[{"name":"stdout","text":"{0: 'OAF_Fear', 1: 'OAF_Pleasant_surprise', 2: 'OAF_Sad', 3: 'OAF_angry', 4: 'OAF_disgust', 5: 'OAF_happy', 6: 'OAF_neutral', 7: 'YAF_angry', 8: 'YAF_disgust', 9: 'YAF_fear', 10: 'YAF_happy', 11: 'YAF_neutral', 12: 'YAF_pleasant_surprised', 13: 'YAF_sad'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"max_len = max([f.shape[0] for f in features])\nprint(max_len)\nx_data = np.array([np.pad(f, ((0, max_len - f.shape[0]), (0, 0)), mode='constant') for f in features])\ny_labels = np.array(labels)\n\n# Split dataset into train, validation, and test sets\nx_train, x_temp, y_train, y_temp = train_test_split(x_data, y_labels, test_size=0.3, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n\n# Convert labels to categorical\ny_train = to_categorical(y_train, len(label_map))\ny_val = to_categorical(y_val, len(label_map))\ny_test = to_categorical(y_test, len(label_map))\n\n# Hyperparameters\ninput_dim = x_data.shape[2]  # Feature size inferred from dataset\nseq_len = x_data.shape[1]  # Sequence length inferred from dataset\nhidden_dim = 128\noutput_dim = len(label_map)  # Number of emotion classes\nnum_layers = 2\nlearning_rate = 0.001\nbatch_size = 32\nepochs = 5\n\n# Define LSTM Model\nmodel = Sequential()\nmodel.add(LSTM(hidden_dim, return_sequences=True, input_shape=(seq_len, input_dim)))\nfor _ in range(num_layers - 1):\n    model.add(LSTM(hidden_dim, return_sequences=False))\nmodel.add(Dense(output_dim, activation='softmax'))\n\n# Compile Model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:05:01.051241Z","iopub.execute_input":"2025-04-04T17:05:01.051722Z","iopub.status.idle":"2025-04-04T17:05:01.333810Z","shell.execute_reply.started":"2025-04-04T17:05:01.051688Z","shell.execute_reply":"2025-04-04T17:05:01.332770Z"}},"outputs":[{"name":"stdout","text":"129\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n\n# Evaluate Model\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\nprint(\"Training Complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:05:01.504111Z","iopub.execute_input":"2025-04-04T17:05:01.504425Z","iopub.status.idle":"2025-04-04T17:05:07.897787Z","shell.execute_reply.started":"2025-04-04T17:05:01.504399Z","shell.execute_reply":"2025-04-04T17:05:07.897082Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.2613 - loss: 2.1541 - val_accuracy: 0.6405 - val_loss: 0.9233\nEpoch 2/5\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7758 - loss: 0.6507 - val_accuracy: 0.8143 - val_loss: 0.4844\nEpoch 3/5\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8771 - loss: 0.3204 - val_accuracy: 0.6690 - val_loss: 0.9111\nEpoch 4/5\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8285 - loss: 0.4511 - val_accuracy: 0.8881 - val_loss: 0.2682\nEpoch 5/5\n\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9265 - loss: 0.1838 - val_accuracy: 0.9548 - val_loss: 0.1463\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9690 - loss: 0.1008\nTest Accuracy: 0.9595\nTraining Complete!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def predict_emotion(file_path):\n    mfcc = extract_features(file_path)\n    mfcc_padded = np.pad(mfcc, ((0, max_len - mfcc.shape[0]), (0, 0)), mode='constant')\n    mfcc_padded = np.expand_dims(mfcc_padded, axis=0)  # Add batch dimension\n    prediction = model.predict(mfcc_padded)\n    predicted_label = np.argmax(prediction)\n    emotion = label_map[predicted_label]\n    print(f\"Predicted Emotion: {emotion}\")\n    return emotion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T17:05:07.898731Z","iopub.execute_input":"2025-04-04T17:05:07.899037Z","iopub.status.idle":"2025-04-04T17:05:07.903412Z","shell.execute_reply.started":"2025-04-04T17:05:07.898996Z","shell.execute_reply":"2025-04-04T17:05:07.902749Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# predict_emotion(\"/kaggle/input/speech-emotion-recognition-en/Tess/YAF_disgust/YAF_back_disgust.wav\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:20:33.268376Z","iopub.execute_input":"2025-04-04T06:20:33.268820Z","iopub.status.idle":"2025-04-04T06:20:33.416355Z","shell.execute_reply.started":"2025-04-04T06:20:33.268787Z","shell.execute_reply":"2025-04-04T06:20:33.414949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.save(\"audio_emotion_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:20:57.058947Z","iopub.execute_input":"2025-04-04T06:20:57.059470Z","iopub.status.idle":"2025-04-04T06:20:57.123714Z","shell.execute_reply.started":"2025-04-04T06:20:57.059421Z","shell.execute_reply":"2025-04-04T06:20:57.122160Z"}},"outputs":[],"execution_count":null}]}